# -*- coding: utf-8 -*-
"""Anna Sophie Gradient Descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECIcS54jn7oB2ixDtSMD2QmH-wcrSpKI
"""

import numpy as np
import math
import matplotlib.pyplot as plt
from scipy.stats import norm
#Function for the Error formula
def Mean_Square_Error(y_measured,y_predicted):
  error = np.sum((y_measured-y_predicted)**2)/len(y_measured)
  return error

#def Standard_Deviation(num_of_points, mean, x, y_measured):
  #error = np.sqrt(np.sum((y_measured-mean)**2)/num_of_points)
  #return error

def Gradient_Descent_Gaussian_Curve(height, width, mean, x, y_measured, iterations, step_size):
  num_of_points = float(len(x))
  first_error = 0
  for i in range(iterations):
    y_predicted = height*(math.e)**(-((x-mean)**2)/(2*(width**2)))
    second_error = Mean_Square_Error(y_measured, y_predicted)
    if abs(first_error-second_error)<=1e-4:
        break
    first_error = second_error
    #Partial derivative for each
    derivative_error_height = -(2/num_of_points)*sum((y_measured-y_predicted)*((math.e)**(-((x-mean)**2)/(2*(width**2)))))
    derivative_error_width = -(2/num_of_points)*sum((y_measured-y_predicted)*(height*((math.e)**(-((x-mean)**2)/(2*(width**2))) * ((x-mean)**2)*((4*width)/((2*(width**2))**2)))))
    derivative_error_mean = -(2/num_of_points)*sum((y_measured-y_predicted)*(height*((math.e)**(-((x-mean)**2)/(2*(width**2))) * (2*(x-mean))/(2*(width**2)))))
    #setting new values
    height = height - (step_size*derivative_error_height)
    width = width - (step_size*derivative_error_width)
    mean = mean - (step_size*derivative_error_mean)
    print(f"iteration:{i+1}")
    print(f"error:{first_error}")
    print(f"height: {height}")
    print(f"width: {width}")
    print(f"mean: {mean}")
  return height, width, mean

def Gradient_Descent(w, b, x, y_measured, step_size, iterations):
  #Number of Scatterplot points in the set
  number_of_points=float(len(x))
  first_error = 0
  for i in range(iterations):
    #What the y values should be based on the line regression so far
    y_predicted = w*x+b
    #Calculating the Mean Square Error using the function
    second_error = Mean_Square_Error(y_measured, y_predicted)
    #Checking if the last two errors are very close in value, then stopping the loop if so
    if abs(first_error-second_error)<=1e-6:
        break
    first_error = second_error
    #Finding the Partial Derivative of the Error function with respect to w and b
    derivative_error_w= -(2/number_of_points) * sum(x*(y_measured-y_predicted))
    derivative_error_b= -(2/number_of_points) * sum(y_measured-y_predicted)
    #Setting new values for w and b based on the partial derivatives
    w = w - (step_size*derivative_error_w)
    b = b - (step_size*derivative_error_b)

    print(f"iteration:{i+1}")
    print(f"error:{first_error}")
    print(f"w: {w}")
    print(f"b: {b}")
  return w, b

def plot_normal_curve(height, mean, width):
  x=-10
  y_predicted = [None]
  for i in range(40):
    y_predicted.append(height*(math.e)**(-((x-mean)**2)/(2*(width**2))))
    x=x+0.5
  return y_predicted

def main():
  #Scatter plot points for x and y
  x = np.array([-10.0, -8.94736842, -7.89473684, -6.84210526, -5.78947368,
  -4.73684211, -3.68421053, -2.63157895, -1.57894737, -0.52631579,
   0.52631579, 1.57894737, 2.63157895, 3.68421053, 4.73684211,
   5.78947368, 6.84210526, 7.89473684, 8.94736842, 10.0])
  y_measured = np.array([0.51670396, 1.52804559, 0.90587027, 0.73178349, 0.93843649, 0.67586433,
  3.1576032, 4.49474599, 6.20748712, 10.5676264, 9.72329015, 8.87819503,
  4.56767258, 2.29275771, 2.41744316, 0.53195457, 1.08641291, 0.7366413,
  1.11104334, 0.55962015])
  x_axis = np.array([-10, -9.5, -9, -8.5, -8, -7.5, -7, -6.5, -6, -5.5, -5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10])
  #Parameters of w, b, step size and iterations
  step_size = 0.01
  iterations = 1000
  height = 11
  width = 4
  mean = 1
  #Finding the line of regression w and b by using the gradient descent function
  final_height, final_width, final_mean = Gradient_Descent_Gaussian_Curve(height, width, mean, x, y_measured, iterations, step_size)
  print(f"{final_height}e^(-((x-{final_mean})^2)/(2*{final_width}^2)))")
  y_predicted = plot_normal_curve(final_height, final_mean, final_width)
  plt.figure(figsize = (8,6))
  plt.scatter(x, y_measured, marker='o', color='red')
  plt.scatter(x_axis, y_predicted, marker='s', color='blue')
  #plt.plot(x_axis, norm.pdf(x, mean, width))
  plt.xlabel("X")
  plt.ylabel("Y")
  plt.show()

if __name__=="__main__":
    main()